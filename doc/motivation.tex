\label{sec:motivation}

Open MPI supports InfiniBand networks through a Byte Transfer Layer
(BTL) plugin module named {\tt openib}.  BTL plugins are the lowest
layer in the Open MPI point-to-point communication stack and are
responsible for actually moving bytes from one MPI process to
another.  The {\tt openib} BTL has both single- and multiple-value
parameters that can be adjusted at run-time.

%Open MPI internally ranks network interconnects based on the best
%latency achievable by that network.  Upon application startup, the MPI
%library determines the list of available networks on each process and
%to each process.  A single process may therefore use multiple network
%interconnects for communication to different processes.  The default
%behavior of Open MPI always chooses the network with the lowest
%latency for short messages.  The ``{\tt btl}'' MCA run-time parameter
%can be used to override this behavior and manually enforce the usage
%of a particular network (or set of networks) for MPI communications,
%For example, to force the use of InfiniBand networks, a user can
%specify {\tt --mca btl openib} on the command line, indicating that
%{\em only} the {\tt openib} BTL module should be used.  Note that this
%will disable all other BTL modules, even potentially lower latency
%networks (such as shared memory, which would have otherwise been used
%for communication on the same node).

There are more than 50 MCA parameters that are related to the {\tt
  openib} BTL module, all of which can be modified at runtime.  Open
MPI attempts to provide reasonable default values for these
parameters, but every application and every platform is different:
maximum performance can only be achieved through tuning for a specific
platform and application behavior.

%
%The following are some of the {\tt openib} BTL's single-value
%parameters that are used to tune Open MPI's short message RDMA
%behavior:\footnote{Note that the exact definition and usage of these
%  parameters are outside the scope of this paper; they are only
%  briefly described here.}

%\begin{itemize}
%\item {\tt btl\_\-openib\_\-ib\_\-max\_\-rdma\_\-dst\_\-opts}: maximum
%  number of outstanding RDMA operations to a specific destination.
%\item {\tt btl\_\-openib\_\-use\_\-eager\_\-rdma}: a logical value
%  specifying whether to use the RDMA protocol for eager messages.
%\item {\tt btl\_\-openib\_\-eager\_\-rdma\_\-threshold}: only use RDMA
%  for short messages to a given peer after this number of messages has
%  been received from that peer.
%\item {\tt btl\_\-openib\_\-max\_\-eager\_\-rdma}: maximum number of
%  peers allowed to use RDMA for short messages.
%\item {\tt btl\_\-openib\_\-eager\_\-rdma\_\-num}: number of RDMA
%  buffers to allocate for short messages.
%\end{itemize}

MPI processes communicate on InfiniBand networks by setting up a pair
of queues to pass messages: one queue for sending and one queue for
receiving.  InfiniBand queues have a large number of attributes and
options that can be used to tailor the behavior of how messages are
passed.  Starting with version v1.3, Open MPI exposes the receive
queue parameters for short messages through the multiple-value
parameter {\tt btl\_\-openib\_\-receive\_\-queues} (long messages use
a different protocol and are governed by a different set of MCA
parameters).  Specifically, this MCA parameter is used to specify one
or more receive queues that will be setup in each MPI process for
InfiniBand communication.  There are two types of receive queues, each
of which have multiple sub-parameters. It is however outside of the scope of this paper to give detailed and precise descriptions of the MCA parameters used. The parameters are:

\begin{enumerate}
\item ``Per-peer'' receive queues are dedicated to receiving messages
  from a single peer MPI process. Per-peer queues have two mandatory
  sub-parameters ({\em size} and {\em num\_\-buffers}) and three
  optional sub-parameters ({\em low\_\-watermark}, {\em
    window\_\-size}, and {\em reserve}).

\item ``Shared'' receive queues are shared between all MPI sending
  processes. Shared receive queues have the same mandatory
  sub-parameters as per-peer receive queues, but have only two
  optional sub-parameters ({\em low\_\-watermark} and {\em
    max\_\-pending\_\-sends}).
\end{enumerate}

The {\tt btl\_\-openib\_\-receive\_\-queues} value is a
colon-delimited listed of queue specifications specifying the queue
type (``P'' or ``S'') and a comma-delimited list of the mandatory and
optional sub-parameters.  For example:

\vspace{5pt}
\centerline{\tt P,128,256,192,128:S,65535,256,128,32}
\vspace{5pt}

will instantiate one per-peer receive queue for each inbound MPI
connection for messages that are $\le128$ bytes, and will setup a
single shared receive queue for all messages that are $>128$ bytes and
$\le65,535$ bytes (messages longer than 65,535 bytes will be handled
by the long message protocol).

Another good example for how to explore the parameter space by OTPO are the tunable values
controlling Open MPI's use of RDMA for short messages.  Short message
RDMA is a resource-intensive, non-scalable optimization for minimizing
point-to-point short message latency.  Finding a good balance between
the desired level of optimization and the resources consumed by this
optimization is exactly the kind of task that OTPO was designed for. Among the most relevant parameters with regard to RDMA operations are {\tt btl\_\-openib\_\-ib\_\-max\_\-rdma\_\-dst\_\-opts}, which limits the maximum number of outstanding RDMA operations to a specific destination; {\tt btl\_\-openib\_\-use\_\-eager\_\-rdma}, a logical value specifying whether to use the RDMA protocol for eager messages; and {\tt btl\_\-openib\_\-eager\_\-rdma\_\-threshold}, only use RDMA for short messages to a given peer after this number of messages has been received from that peer. Due to space limitations, we will not detail all RDMA parameters or present RDMA results of the according OTPO runs.

%%% Local Variables: 
%%% mode: latex
%%% TeX-master: "paper"
%%% End: 
