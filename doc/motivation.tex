\label{sec:motivation}

OTPO supports testing two general types of MCA parameters:

\begin{enumerate}
\item Single-value parameters: these parameters represent an
  individual value, such as an integer.
\item Multiple-value parameters: these parameters are composed of
  one or more sub-parameters, each of which can vary.
\end{enumerate}

In this section, we will describe some of the tunable values of the
version 1.3 series of Open MPI's {\tt openib} BTL module -- both
single- and multiple-value -- and explain how OTPO can systematically
search the parameter space.

Open MPI internally ranks network interconnects based on the best
latency achievable by that network. Upon application startup, the MPI
library determines the list of available networks on each process and
to each process.  As such, the same process might use multiple network
interconnects for communication to different processes. The default
behavior of Open MPI always chooses the network with the lowest
latency for short messages.  The ``{\tt btl}'' MCA run-time parameter
can be used to override this behavior and manually enforce the usage
of a particular network (or set of networks) for MPI communications,
For example, to force the use of InfiniBand networks, a user can
specify {\tt --mca btl openib} on the command line, indicating that
{\em only} the {\tt openib} BTL module should be used.  Note that this
will disable all other BTL modules, even potentially lower latency
networks (such as shared memory, which would have otherwise been used
for communication on the same node).

There are more than 50 MCA parameters that are related to the {\tt
  openib} BTL module, all of which can be modified at runtime.  Open
MPI attempts to provide reasonable default values for these
parameters, but every application and every platform is different:
maximum performance can only be achived through tuning for a specific
platform and application behavior.

A good parameter space for OTPO to search is the tunable values
controlling Open MPI's use of RDMA for short messages.  Short message
RDMA is a resource-intensive, non-scalable optimization for minimizing
point-to-point short message latency.  Finding a good balance between
the desired level of optimization and the resources consumed by this
optimization is exactly the kind of task that OTPO was designed for.

The following are some of the {\tt openib} BTL's single-value
parameters that are used to tune Open MPI's short message RDMA
behavior:\footnote{Note that the exact definition and usage of these
  parameters are outside the scope of this paper; they are only
  briefly described.}

\begin{itemize}
\item {\tt btl\_\-openib\_\-ib\_\-max\_\-rdma\_\-dst\_\-opts}: maximum
  number of outstanding RDMA operations to a specific destination.
\item {\tt btl\_\-openib\_\-use\_\-eager\_\-rdma}: a logical value
  specifying whether to use the RDMA protocol for eager messages.
\item {\tt btl\_\-openib\_\-eager\_\-rdma\_\-threshold}: only use RDMA
  for short messages to a given peer after this number of messages has
  been received from that peer.
\item {\tt btl\_\-openib\_\-max\_\-eager\_\-rdma}: maximum number of
  peers allowed to use RDMA for short messages.
\item {\tt btl\_\-openib\_\-eager\_\-rdma\_\-num}: number of RDMA
  buffers to allocate for short messages.
\end{itemize}

MPI processes communication on InfiniBand networks by setting up a
pair of queues to pass messages: one queue for sending and one queue
for receiving.  InfiniBand queues have a large number of attributes
and options that can be used to tailor the behavior of how messages
are passed.  Starting with version v1.3, Open MPI exposes the receive
queue parameters through the multiple-value parameter {\tt
  btl\_\-openib\_\-receive\_\-queues}.  Specifically, this MCA
parameter is used to specify one or more receive queues that will be
setup in each MPI process for InfiniBand communication.  There are two
types of receive queues, each of which have multiple
sub-parameters:\footnote{As with the eager RDMA parameters, the {\tt
    btl\_\-openib\_\-receive\_\-queues} MCA parameter is only loosely
  described in this paper.}

\begin{enumerate}
\item ``Per-peer'' receive queues are dedicated to receiving messages
  from a single peer MPI process. Per-peer queues have two mandatory
  sub-parameters ({\em size} and {\em num\_\-buffers}) and three
  optional sub-parameters ({\em low\_\-watermark}, {\em
    window\_\-size}, and {\em reserve}).

\item ``Shared'' receive queues are shared between all MPI sending
  processes. Shared receive queues have the same mandatory
  sub-parameters as per-peer receive queues, but have two optional
  sub-parameters ({\em low\_\-watermark} and {\em
    max\_\-pending\_\-sends}).
\end{enumerate}

%%% Local Variables: 
%%% mode: latex
%%% TeX-master: "paper"
%%% End: 
