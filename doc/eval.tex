{\Large JMS: This section needs a lot of work; it does not make sense if
  you don't know anything about IB or Open MPI}

This section presents an example of using OTPO in order to optimize
the InfiniBand parameters of Open MPI on a given platform. Therefore, we first discuss the parameters of the {\tt openib} BTL module in Open MPI, than present the results of the optimization using OTPO.

\subsection{InfiniBand parameters in Open MPI}
%\label{sec:mot}
\input{motivation}

\subsection{Results}
The test was run on the shark cluster at the University of Houston. Shark
consists of 24 dual-core 2.2GHz AMD Opteron nodes connected by a 4x
InfiniBand and a Gigabit Ethernet network interconnect. 
The MCA parameters used for the evaluation is a subset of the
parameters mentioned in section~\ref{sec:motivation}, namely {\tt
  btl\_openib\_receive\_queues}. As discussed in
section~\ref{sec:motivation}, this parameter is the specification of
one or more receive queues, each with several sub-parameters.  In
order for OTPO to handle this parameter, it has to be specified as an
aggregate of other ``virtual'' parameters. In our test, we decided to
use two receive queues: one per-peer queue and one shared receive
queue. The following specification was used for the MCA parameter:

\begin{itemize}
\item {\tt btl\_openib\_rq\_1\_type}: P
\item {\tt btl\_openib\_rq\_1\_size}: 65536 $\rightarrow$ 262144
  (increment * 2)
\item {\tt btl\_openib\_rq\_1\_num}: 1 $\rightarrow$ 256 (increment *
  2)
\item {\tt btl\_openib\_rq\_1\_low\_wat}: 1 $\rightarrow$ 64
  (increment * 2)
\item {\tt btl\_openib\_rq\_2\_type}: S
\item {\tt btl\_openib\_rq\_2\_size}: 65536 $\rightarrow$ 262144
  (increment * 2)
\item {\tt btl\_openib\_rq\_2\_num}: 1 $\rightarrow$ 256 (increment *
  2)
\item {\tt btl\_openib\_rq\_2\_low\_wat}: 1 $\rightarrow$ 64
  (increment * 2)
\item {\tt btl\_openib\_rq\_2\_max\_pending\_sends}: 1 $\rightarrow$
  32 (increment * 2)
\item {\tt btl\_openib\_receive\_queues}: aggregate of the above
  parameters
\end{itemize}

This configuration yields 37,730 combinations, after removing
unnecessary combinations that would lead to wrong or inaccurate
results.  Using the brute force method, OTPO took over 6.5 hours to
test all of these combinations.

\begin{table}[tb]
\centering
\begin{tabular}{|c|c|} \hline
Latency & Number of Combinations \\
\hline
3.87$\mu s$  & 1\\
\hline
3.88$\mu s$  & 619\\
\hline
3.89$\mu s$  & 9,702\\
\hline
3.90$\mu s$  & 10,977\\
\hline
3.91$\mu s$  & 7,907\\
\hline
3.92$\mu s$  & 5,076\\
\hline
\end{tabular}  
\caption{OTPO results of the best parameter combinations}
\label{table:results} 
\end{table}

The results that are summarized in Table~\ref{table:results} show the
number of combinations that are in the range of 0.05$\mu s$ of the
best latency. The best possible combination was with the following
combination of values:
$P,131072,64,4:S,262144,256,64,16$\\
The results suggest the best attributes for the two queue pairs:\\
Per-peer queue:
\begin{itemize}
\item The size of the receive buffers to be posted is 131,072 bytes.
\item The maximum number of buffers posted for incoming message
  fragments is 64.
\item the number of available buffers left on the queue before Open
  MPI reposts buffers up to the maximum (previous parameter) is 4.
\end{itemize}
Shared Receive Queue:
\begin{itemize}
\item The size of the receive buffers to be posted is 262,144 bytes.
\item The maximum number of buffers posted for incoming message
  fragments is 256.
\item the number of available buffers left on the queue before Open
  MPI reposts buffers up to the maximum (previous parameter) is 64.
\item the maximum number of outstanding sends that are allowed at a
  given time is 16.
\end{itemize}

%%% Local Variables: 
%%% mode: latex
%%% TeX-master: "paper"
%%% End: 
