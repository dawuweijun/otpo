This section presents an example using OTPO to optimize some of the
InfiniBand parameters of Open MPI on a given platform.  We therefore
first describe Open MPI's InfiniBand support and some of its run-time
tunable parameters, then present the results of the optimization using
OTPO.

\subsection{InfiniBand Parameters in Open MPI}

\input{motivation}

\subsection{Results}

Tests were run on the Shark cluster at the University of Houston.
Shark consists of 24 dual-core 2.2GHz AMD Opteron nodes connected by
4x InfiniBand and Gigabit Ethernet network interconnects.  
The InfiniBand switch is connected to two HCAs on every node, of which one is
active, with an {\tt active\_mtu} of 2048 and an {\tt active\_speed} of 2.5
Gbps. OFED 1.1 is installed on the nodes. 

A pre-release version of Open MPI v1.3 was used to generate these
results, Subversion trunk revision 16924. A nightly snapshot of the trunk was
used, and configured with debug disabled. All the tests were run with
{\tt MPI\_LEAVE\_PINNED} set to one.

OTPO was used to explore the parameter space of {\tt
  btl\_\-openib\_\-receive\_\-queues} to find a set of values that
yield the lowest short message latency.  Since {\tt receive\_\-queues}
is a multiple-value parameter, each sub-parameter must be described to
OTPO.  The individual sub-parameters become ``virtual'' parameters,
each with a designated range to explore.  OTPO was configured to test
both a per-peer and a shared receive queue with the ranges listed in
Table~\ref{table:eval-queue-search-params}.  Each sub-parameter
spanned its range by doubling its value from the minimum to the
maximum (e.g., 1, 2, 4, 8, 16, ...).

\def\yes{$\sqrt{}$}

\begin{table}[tb]
\centering
\caption{InfiniBand receive queue search parameter ranges.  The ``max
  pending sends'' sub-parameter is only relevant for shared receive
  queues.}
\label{table:eval-queue-search-params} 
\begin{tabular}{|l|c|c|c|} 
\multicolumn{1}{c}{Sub-parameter} &
\multicolumn{1}{c}{Range} &
\multicolumn{1}{c}{Per-peer} &
\multicolumn{1}{c}{Shared} \\
\hline
Buffer size (bytes) & 65,536 $\rightarrow$ 262,144 & \yes & \yes \\
Number of buffers & 1 $\rightarrow$ 256 & \yes & \yes \\
Low watermark (buffers) & 1 $\rightarrow$ 64  & \yes & \yes \\
Max pending sends & 1 $\rightarrow$ 32 & & \yes \\
\hline
\end{tabular}
\end{table}

Even with this coarse-grained sampling of the parameter space, the
five-dimensional parameter space from
Table~\ref{table:eval-queue-search-params} yields 37,730 valid combinations
(after removing unnecessary combinations that would cause to incorrect
results). These combinations stressed buffer management and flow control
issues in the Open MPI short message protocol when sending 1 byte
messages. Using the brute force method, OTPO took over 6.5 hours to invoke
NetPIPE for each of these parameter combinations.  Note that NetPIPE runs each
0-byte ping-pong test 1,000 times and reports half the average round-trip
time.  OTPO sought parameter sets that minimized this value.

\begin{table}[tb]
\centering
\caption{OTPO results of the best parameter combinations.}
\label{table:results} 
\begin{tabular}{|c|c|} \hline
Latency & Number of Combinations \\
\hline
3.87$\mu s$  & 1\\
\hline
3.88$\mu s$  & 619\\
\hline
3.89$\mu s$  & 9,702\\
\hline
3.90$\mu s$  & 10,977\\
\hline
3.91$\mu s$  & 7,907\\
\hline
3.92$\mu s$  & 5,076\\
\hline
\end{tabular}  
\end{table}

The results are summarized in Table~\ref{table:results}.  Although
there was exactly one parameter set that resulted in the lowest
latency ($3.87\mu s$), there were thousands that were within $0.05\mu
s$.  With timings this low, jitter within the results is to be
expected.  

{\Large Mohamad: I'm confused as to what was actually run...}

{\em Were you testing {\bf 2} receive queues?  I.e., ``P....:S....''?
  The best possible param set listed below suggests this.  If so, why?
  0 byte latency is not affected by the 2nd queue -- it will only use
  the smallest message queue.  Were you testing more than just 0 byte
  latency?  If so, what exactly were you testing?}

{\Large I'm quite suspicious of the fact that there was only {\bf 1}
  best result.  If you run the tests again, are the results
  consistent?}

{\em .....I cannot edit the following without answers to the above
  questions...}

 the
number of combinations that are in the range of 0.05$\mu s$ of the
best latency. The best possible combination was with the following
combination of values:
$P,131072,64,4:S,262144,256,64,16$\\
The results suggest the best attributes for the two queue pairs:\\
Per-peer queue:
\begin{itemize}
\item The size of the receive buffers to be posted is 131,072 bytes.
\item The maximum number of buffers posted for incoming message
  fragments is 64.
\item the number of available buffers left on the queue before Open
  MPI reposts buffers up to the maximum (previous parameter) is 4.
\end{itemize}
Shared Receive Queue:
\begin{itemize}
\item The size of the receive buffers to be posted is 262,144 bytes.
\item The maximum number of buffers posted for incoming message
  fragments is 256.
\item the number of available buffers left on the queue before Open
  MPI reposts buffers up to the maximum (previous parameter) is 64.
\item the maximum number of outstanding sends that are allowed at a
  given time is 16.
\end{itemize}

%%% Local Variables: 
%%% mode: latex
%%% TeX-master: "paper"
%%% End: 
