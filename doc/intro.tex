In the current top 500 list~\cite{top500} clustered high performance computing systems clearly dominate from the architectural perspective. Off-the-shelf components make clusters attractive for low-end budgets as well as for large scale installations, since they offer the opportunity to customize the equipment according to their needs and financial constraints. However, the flexibility comes at the price: the performance that end-users experience with real-world applications deviates significantly from the theoretical peak performance of the cluster. This is mainly due to the fact, that each system represents a nearly unique execution environment. Typically, neither software nor hardware components have been hand-tuned to that particular combination of processors, network interconnects and software environment. %Moreover, statically pre-tuning of software components is only of limited value, since the performance of an application will also depend on runtime-options such as process placement, resource utilization and application specific parameters.

In order to optimize the performance of a particular system, research groups have turned to extensive pre-execution tuning. As an example, the ATLAS project~\cite{atlas} evaluates in a configure step a large number of implementation possibilities for the core loops of the BLAS routines. Similarly, the Automatically Tuned Collective Communication project~\cite{pjesa:cluster} incorporates an exhaustive search in order to determine the best performing algorithms for a wide range of message length for MPI's collective operations.  The FFTW library~\cite{fftw} tunes fast fourier transform operations (FFT) in a so-called planner step before executing the FFT operations of the actual application.

One critical piece of software has however not been systematically approached in any of the projects. MPI libraries represent the interface between most parallel applications and the hardware today. Libraries such as MPICH~\cite{Gropp:1996:HPI} and Open MPI~\cite{gabriel:ompi} provide flexible and tunable implementations, which can be adapted either at compile or at runtime to a particular environment. In this paper, we introduce OTPO (Open Tool for Parameter Optimization), a new tool developed in partnership between Cisco Systems and the University of Houston. OTPO is an Open MPI specific tool aiming at optimizing parameters of the runtime environment exposed through the MPI library to the end-user application. Since parameters might expose explicit or implicit dependencies among each other, some of them possibly even unknown to the module developers, OTPO aims at systematically discovering those hidden dependencies and the effect they have on overall performance, such as the point-to-point latency or bandwidth. We present the current status of OTPO and the ongoing work.

The rest of the paper is organized as follows: section~\ref{sec:concept}
presents the concept and the architecture of OTPO. 
Section~\ref{sec:impl} discusses some implementation details
of OTPO. In section~\ref{sec:eval}, OTPO is used to explore the
parameter space of Open MPI's short message protocol in order to
minimize latency on InfiniBand networks. Finally,
section~\ref{sec:summary} summarizes 
the paper and discusses ongoing work.

